[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FC_AGENT üëÄ",
    "section": "",
    "text": "This notebook is used for fine tuning the model for function calling agents. Base models are taken from huggingface model hub. These are the models that can be used for fine tuning.  1- https://huggingface.co/google/gemma-2-2b-it  2- https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct  3- https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct \nAll models are already fine tuned on instruct dataset. The reason I chose these is that I need to update less information to fine tune for function calling instead of base models. I will be using LoRA for fine tuning. \n\n\n\n! pip install -q -U bitsandbytes   # for quantization\n! pip install -q -U peft           # for LoRA adapters\n! pip install -q -U trl            # for loading the model\n! pip install -q -U tensorboardX   # for loading and using the fine-tuning dataset\n! pip install -q wandb             # for the trainer class\n\n\nfrom enum import Enum\nfrom functools import partial\nimport pandas as pd\nimport torch\nimport json\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\nfrom datasets import load_dataset\nfrom trl import SFTConfig, SFTTrainer\nfrom peft import LoraConfig, TaskType\n\nseed = 42\nset_seed(seed)\n\nimport os\n\n# Put your HF Token here\nos.environ['HF_TOKEN']=\"hf_xxxxxxx\" # the token should have write access\n\n\n\n\nWe need to format the inputs into what we want the model to learn.\nDataset for this fine tuning contains: ‚ÄúNousResearch/hermes-function-calling-v1‚Äù (A popular dataset for function calling) and some of the dataset new thinking step computer from deepseek-ai/DeepSeek-R1-Distill-Qwen-32B.\nDefault chat_template of gemma-2-2B does not contain tool calls Conversation format needs to be changed to include tool calls.\n‚ÄúNousResearch/hermes-function-calling-v1‚Äù does not include a ‚Äúthinking‚Äù step. In Function-Calling, such a step is optional, but recent work‚Äîlike the deepseek model or the paper ‚ÄúTest-Time Compute‚Äù ‚Äî suggests that giving an LLM time to ‚Äúthink‚Äù before it answers (or in this case, before taking an action) can significantly improve model performance.\n\nmodel_name = \"google/gemma-2-2b-it\"\ndataset_name = \"Jofthomas/hermes-function-calling-thinking-V1\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ntokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '&lt;start_of_turn&gt;' + message['role'] + '\\n' + message['content'] | trim + '&lt;end_of_turn&gt;&lt;eos&gt;\\n' }}{% endfor %}{% if add_generation_prompt %}{{'&lt;start_of_turn&gt;model\\n'}}{% endif %}\"\n\n\ndef preprocess(sample):\n      messages = sample[\"messages\"]\n      first_message = messages[0]\n\n      # Instead of adding a system message, we merge the content into the first user message\n      if first_message[\"role\"] == \"system\":\n          system_message_content = first_message[\"content\"]\n          # Merge system content with the first user message\n          messages[1][\"content\"] = system_message_content + \"Also, before making a call to a function take the time to plan the function to take. Make that thinking process between &lt;think&gt;{your thoughts}&lt;/think&gt;\\n\\n\" + messages[1][\"content\"]\n          # Remove the system message from the conversation\n          messages.pop(0)\n\n      return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n\n\n\ndataset = load_dataset(dataset_name)\ndataset = dataset.rename_column(\"conversations\", \"messages\")\n\nExample of the conversation format:\n&lt;bos&gt;&lt;start_of_turn&gt;human\nYou are a function calling AI model. You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:&lt;tools&gt; [{'type': 'function', 'function': {'name': 'get_news_headlines', 'description': 'Get the latest news headlines', 'parameters': {'type': 'object', 'properties': {'country': {'type': 'string', 'description': 'The country for which headlines are needed'}}, 'required': ['country']}}}, {'type': 'function', 'function': {'name': 'search_recipes', 'description': 'Search for recipes based on ingredients', 'parameters': {'type': 'object', 'properties': {'ingredients': {'type': 'array', 'items': {'type': 'string'}, 'description': 'The list of ingredients'}}, 'required': ['ingredients']}}}] &lt;/tools&gt;Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags as follows:\n&lt;tool_call&gt;\n{tool_call}\n&lt;/tool_call&gt;Also, before making a call to a function take the time to plan the function to take. Make that thinking process between &lt;think&gt;{your thoughts}&lt;/think&gt;\n\nCan you get me the latest news headlines for the United States?&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\n&lt;think&gt;Okay, so the user asked for the latest news headlines in the United States. I need to figure out how to respond appropriately using the tools available.\n\nLooking at the tools provided, there's a function called get_news_headlines. That seems perfect for this situation. It requires a country code, which the user specified as the United States.\n\nSince the user is asking specifically about the US, I'll use the country parameter as 'United States' in the function call. This should fetch the relevant news for them.\n\nI should make sure to format the response correctly as a JSON object with the function name and arguments.\n&lt;/think&gt;&lt;tool_call&gt;\n{'name': 'get_news_headlines', 'arguments': {'country': 'United States'}}\n&lt;/tool_call&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;tool\n&lt;tool_response&gt;\n{'headlines': ['US President signs new bill into law', 'Major earthquake hits California', 'NASA announces new Mars mission', 'Tech giant releases new smartphone', 'US economy shows signs of recovery']}\n&lt;/tool_response&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\nHere are the latest news headlines for the United States:\n1. US President signs new bill into law\n2. Major earthquake hits California\n3. NASA announces new Mars mission\n4. Tech giant releases new smartphone\n5. US economy shows signs of recovery&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;human\nThat's interesting. Can you get me the news for France now?&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\n&lt;think&gt;Okay, the user initially asked for the latest US news, so I used get_news_headlines with 'United States'. Now the user is asking for France, so I'll call the same function with 'France' as the country. That makes sense because the function is designed for any country.\n&lt;/think&gt;&lt;tool_call&gt;\n{'name': 'get_news_headlines', 'arguments': {'country': 'France'}}\n&lt;/tool_call&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;tool\n&lt;tool_response&gt;\n{'headlines': ['French President announces new environmental policy', 'Paris Fashion Week highlights', 'France wins World Cup qualifier', 'New culinary trend sweeps across France', 'French tech startup raises millions in funding']}\n&lt;/tool_response&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\nHere are the latest news headlines for France:\n1. French President announces new environmental policy\n2. Paris Fashion Week highlights\n3. France wins World Cup qualifier\n4. New culinary trend sweeps across France\n5. French tech startup raises millions in funding&lt;end_of_turn&gt;&lt;eos&gt;\nTokenizer splits text into sub-words by default. This is not what we want for our new special tokens!\nWhile we segmented our example using &lt;think&gt;, &lt;tool_call&gt;, and &lt;tool_response&gt;, the tokenizer does not yet treat them as whole tokens‚Äîit still tries to break them down into smaller pieces. To ensure the model correctly interprets our new format, we must add these tokens to our tokenizer.\nAdditionally, since we changed the chat_template in our preprocess function to format conversations as messages within a prompt, we also need to modify the chat_template in the tokenizer to reflect these changes.\n\ndataset = dataset.map(preprocess, remove_columns=\"messages\")\ndataset = dataset[\"train\"].train_test_split(0.1)\nprint(dataset)\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 3213\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 357\n    })\n})\n\n\n\n\n\nLet‚Äôs manually look at what an input looks like !\nIn this example we have :\n\nA User message containing the necessary information with the list of available tools inbetween &lt;tools&gt;&lt;/tools&gt; then the user query, here: \"Can you get me the latest news headlines for the United States?\"\nAn Assistant message here called ‚Äúmodel‚Äù to fit the criterias from gemma models containing two new phases, a ‚Äúthinking‚Äù phase contained in &lt;think&gt;&lt;/think&gt; and an ‚ÄúAct‚Äù phase contained in &lt;tool_call&gt;&lt;/&lt;tool_call&gt;.\nIf the model contains a &lt;tools_call&gt;, we will append the result of this action in a new ‚ÄúTool‚Äù message containing a &lt;tool_response&gt;&lt;/tool_response&gt; with the answer from the tool.\n\n\n# Let's look at how we formatted the dataset\nprint(dataset[\"train\"][8][\"text\"])\n\n&lt;bos&gt;&lt;start_of_turn&gt;human\nYou are a function calling AI model. You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:&lt;tools&gt; [{'type': 'function', 'function': {'name': 'get_news_headlines', 'description': 'Get the latest news headlines', 'parameters': {'type': 'object', 'properties': {'country': {'type': 'string', 'description': 'The country for which headlines are needed'}}, 'required': ['country']}}}, {'type': 'function', 'function': {'name': 'search_recipes', 'description': 'Search for recipes based on ingredients', 'parameters': {'type': 'object', 'properties': {'ingredients': {'type': 'array', 'items': {'type': 'string'}, 'description': 'The list of ingredients'}}, 'required': ['ingredients']}}}] &lt;/tools&gt;Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags as follows:\n&lt;tool_call&gt;\n{tool_call}\n&lt;/tool_call&gt;Also, before making a call to a function take the time to plan the function to take. Make that thinking process between &lt;think&gt;{your thoughts}&lt;/think&gt;\n\nCan you get me the latest news headlines for the United States?&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\n&lt;think&gt;Okay, so the user asked for the latest news headlines in the United States. I need to figure out how to respond appropriately using the tools available.\n\nLooking at the tools provided, there's a function called get_news_headlines. That seems perfect for this situation. It requires a country code, which the user specified as the United States.\n\nSince the user is asking specifically about the US, I'll use the country parameter as 'United States' in the function call. This should fetch the relevant news for them.\n\nI should make sure to format the response correctly as a JSON object with the function name and arguments.\n&lt;/think&gt;&lt;tool_call&gt;\n{'name': 'get_news_headlines', 'arguments': {'country': 'United States'}}\n&lt;/tool_call&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;tool\n&lt;tool_response&gt;\n{'headlines': ['US President signs new bill into law', 'Major earthquake hits California', 'NASA announces new Mars mission', 'Tech giant releases new smartphone', 'US economy shows signs of recovery']}\n&lt;/tool_response&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\nHere are the latest news headlines for the United States:\n1. US President signs new bill into law\n2. Major earthquake hits California\n3. NASA announces new Mars mission\n4. Tech giant releases new smartphone\n5. US economy shows signs of recovery&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;human\nThat's interesting. Can you get me the news for France now?&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\n&lt;think&gt;Okay, the user initially asked for the latest US news, so I used get_news_headlines with 'United States'. Now the user is asking for France, so I'll call the same function with 'France' as the country. That makes sense because the function is designed for any country.\n&lt;/think&gt;&lt;tool_call&gt;\n{'name': 'get_news_headlines', 'arguments': {'country': 'France'}}\n&lt;/tool_call&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;tool\n&lt;tool_response&gt;\n{'headlines': ['French President announces new environmental policy', 'Paris Fashion Week highlights', 'France wins World Cup qualifier', 'New culinary trend sweeps across France', 'French tech startup raises millions in funding']}\n&lt;/tool_response&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\nHere are the latest news headlines for France:\n1. French President announces new environmental policy\n2. Paris Fashion Week highlights\n3. France wins World Cup qualifier\n4. New culinary trend sweeps across France\n5. French tech startup raises millions in funding&lt;end_of_turn&gt;&lt;eos&gt;\n\n\n\n\n# Sanity check\nprint(tokenizer.pad_token)\nprint(tokenizer.eos_token)\n\n&lt;pad&gt;\n&lt;eos&gt;\n\n\n\n\n\nTokenizer splits text into sub-words by default. This is not what we want for our new special tokens!\nWhile we segmented our example using &lt;think&gt;, &lt;tool_call&gt;, and &lt;tool_response&gt;, the tokenizer does not yet treat them as whole tokens‚Äîit still tries to break them down into smaller pieces. To ensure the model correctly interprets our new format, we must add these tokens to our tokenizer.\nAdditionally, since we changed the chat_template in our preprocess function to format conversations as messages within a prompt, we also need to modify the chat_template in the tokenizer to reflect these changes.\n\nclass ChatmlSpecialTokens(str, Enum):\n    tools = \"&lt;tools&gt;\"\n    eotools = \"&lt;/tools&gt;\"\n    think = \"&lt;think&gt;\"\n    eothink = \"&lt;/think&gt;\"\n    tool_call=\"&lt;tool_call&gt;\"\n    eotool_call=\"&lt;/tool_call&gt;\"\n    tool_response=\"&lt;tool_reponse&gt;\"\n    eotool_response=\"&lt;/tool_reponse&gt;\"\n    pad_token = \"&lt;pad&gt;\"\n    eos_token = \"&lt;eos&gt;\"\n    @classmethod\n    def list(cls):\n        return [c.value for c in cls]\n\ntokenizer = AutoTokenizer.from_pretrained(\n        model_name,\n        pad_token=ChatmlSpecialTokens.pad_token.value,\n        additional_special_tokens=ChatmlSpecialTokens.list()\n    )\ntokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '&lt;start_of_turn&gt;' + message['role'] + '\\n' + message['content'] | trim + '&lt;end_of_turn&gt;&lt;eos&gt;\\n' }}{% endfor %}{% if add_generation_prompt %}{{'&lt;start_of_turn&gt;model\\n'}}{% endif %}\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                             attn_implementation='eager',\n                                             device_map=\"auto\")\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(torch.bfloat16)\n\n\n\n\nThe new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n\n\nGemma2ForCausalLM(\n  (model): Gemma2Model(\n    (embed_tokens): Embedding(256008, 2304, padding_idx=0)\n    (layers): ModuleList(\n      (0-25): 26 x Gemma2DecoderLayer(\n        (self_attn): Gemma2Attention(\n          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n          (rotary_emb): Gemma2RotaryEmbedding()\n        )\n        (mlp): Gemma2MLP(\n          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n          (act_fn): PytorchGELUTanh()\n        )\n        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n      )\n    )\n    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n  )\n  (lm_head): Linear(in_features=2304, out_features=256008, bias=False)\n)\n\n\n\n\n\nThis is we are going to define the parameter of our adapter. Those a the most important parameters in LoRA as they define the size and importance of the adapters we are training.\n\nfrom peft import LoraConfig\n\n# TODO: Configure LoRA parameters\n# r: rank dimension for LoRA update matrices (smaller = more compression)\nrank_dimension = 16\n# lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)\nlora_alpha = 64\n# lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)\nlora_dropout = 0.05\n\npeft_config = LoraConfig(r=rank_dimension,\n                         lora_alpha=lora_alpha,\n                         lora_dropout=lora_dropout,\n                         target_modules=[\"gate_proj\",\"q_proj\",\"lm_head\",\"o_proj\",\"k_proj\",\"embed_tokens\",\"down_proj\",\"up_proj\",\"v_proj\"], # wich layer in the transformers do we target ?\n                         task_type=TaskType.CAUSAL_LM)\n\n\n\n\nTrainer, the class that we use to fine-tune our model and the hyperparameters.\n\nusername=\"Jofthomas\"# REPLCAE with your Hugging Face username\noutput_dir = \"gemma-2-2B-it-thinking-function_calling-V0\" # The directory where the trained model checkpoints, logs, and other artifacts will be saved. It will also be the default name of the model when pushed to the hub if not redefined later.\nper_device_train_batch_size = 1\nper_device_eval_batch_size = 1\ngradient_accumulation_steps = 4\nlogging_steps = 5\nlearning_rate = 1e-4 # The initial learning rate for the optimizer.\n\nmax_grad_norm = 1.0\nnum_train_epochs=1\nwarmup_ratio = 0.1\nlr_scheduler_type = \"cosine\"\nmax_seq_length = 1500\n\ntraining_arguments = SFTConfig(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    per_device_eval_batch_size=per_device_eval_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    save_strategy=\"no\",\n    eval_strategy=\"epoch\",\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    max_grad_norm=max_grad_norm,\n    weight_decay=0.1,\n    warmup_ratio=warmup_ratio,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\",\n    bf16=True,\n    hub_private_repo=False,\n    push_to_hub=False,\n    num_train_epochs=num_train_epochs,\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    packing=True,\n    max_seq_length=max_seq_length,\n)\n\nAs Trainer, we use the SFTTrainer which is a Supervised Fine-Tuning Trainer.\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_arguments,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    processing_class=tokenizer,\n    peft_config=peft_config,\n)\n\n/home/user/miniconda/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:543: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we launch the training üî•. Perfect time for you to pause and grab a coffee ‚òï.\n\ntrainer.train()\ntrainer.save_model()\n\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n\n\n\n    \n      \n      \n      [12/12 00:24, Epoch 1/1]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\n\n\n\n\n1\n1.236800\n1.240833\n\n\n\n\n\n\n/home/user/miniconda/lib/python3.9/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n\n\n\n\n\nTo so:\n\nLoad the adapter from the hub !\nLoad the base model : ‚Äúgoogle/gemma-2-2b-it‚Äù from the hub\nResize the model to with the new tokens we introduced !\n\n\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom datasets import load_dataset\nimport torch\n\nbnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n            bnb_4bit_use_double_quant=True,\n        )\n\npeft_model_id = f\"{username}/{output_dir}\" # replace with your newly trained adapter\ndevice = \"auto\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n                                             device_map=\"auto\",\n                                             )\ntokenizer = AutoTokenizer.from_pretrained(peft_model_id)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel = PeftModel.from_pretrained(model, peft_model_id)\nmodel.to(torch.bfloat16)\nmodel.eval()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Gemma2ForCausalLM(\n      (model): Gemma2Model(\n        (embed_tokens): lora.Embedding(\n          (base_layer): Embedding(256006, 2304, padding_idx=0)\n          (lora_dropout): ModuleDict(\n            (default): Dropout(p=0.05, inplace=False)\n          )\n          (lora_A): ModuleDict()\n          (lora_B): ModuleDict()\n          (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 16x256006 (cuda:0)])\n          (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 2304x16 (cuda:0)])\n          (lora_magnitude_vector): ModuleDict()\n        )\n        (layers): ModuleList(\n          (0-25): 26 x Gemma2DecoderLayer(\n            (self_attn): Gemma2Attention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=2304, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear(\n                (base_layer): Linear(in_features=2304, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=2304, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=2304, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2304, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): Gemma2RotaryEmbedding()\n            )\n            (mlp): Gemma2MLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=2304, out_features=9216, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=9216, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=2304, out_features=9216, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=9216, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=9216, out_features=2304, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=9216, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2304, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): PytorchGELUTanh()\n            )\n            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n          )\n        )\n        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n      )\n      (lm_head): lora.Linear(\n        (base_layer): Linear(in_features=2304, out_features=256006, bias=False)\n        (lora_dropout): ModuleDict(\n          (default): Dropout(p=0.05, inplace=False)\n        )\n        (lora_A): ModuleDict(\n          (default): Linear(in_features=2304, out_features=16, bias=False)\n        )\n        (lora_B): ModuleDict(\n          (default): Linear(in_features=16, out_features=256006, bias=False)\n        )\n        (lora_embedding_A): ParameterDict()\n        (lora_embedding_B): ParameterDict()\n        (lora_magnitude_vector): ModuleDict()\n      )\n    )\n  )\n)\n\n\n\nprint(dataset[\"test\"][8][\"text\"])\n\n&lt;bos&gt;&lt;start_of_turn&gt;human\nYou are a function calling AI model. You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:&lt;tools&gt; [{'type': 'function', 'function': {'name': 'convert_currency', 'description': 'Convert from one currency to another', 'parameters': {'type': 'object', 'properties': {'amount': {'type': 'number', 'description': 'The amount to convert'}, 'from_currency': {'type': 'string', 'description': 'The currency to convert from'}, 'to_currency': {'type': 'string', 'description': 'The currency to convert to'}}, 'required': ['amount', 'from_currency', 'to_currency']}}}, {'type': 'function', 'function': {'name': 'calculate_distance', 'description': 'Calculate the distance between two locations', 'parameters': {'type': 'object', 'properties': {'start_location': {'type': 'string', 'description': 'The starting location'}, 'end_location': {'type': 'string', 'description': 'The ending location'}}, 'required': ['start_location', 'end_location']}}}] &lt;/tools&gt;Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags as follows:\n&lt;tool_call&gt;\n{tool_call}\n&lt;/tool_call&gt;Also, before making a call to a function take the time to plan the function to take. Make that thinking process between &lt;think&gt;{your thoughts}&lt;/think&gt;\n\nHi, I need to convert 500 USD to Euros. Can you help me with that?&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\n&lt;think&gt;Alright, so I'm looking at this scenario where the user needs help converting 500 USD to Euros. Let's break it down step by step to understand how to approach this.\n\nFirst, the user has asked for conversion from USD to EUR. I know there's a specific tool available for currency conversion called `convert_currency`. This tool requires three parameters: amount, from_currency, and to_currency. In the user's message, they've provided all necessary details‚Äî500 USD and the desire to convert to Euros. \n\nI don't need to make any assumptions here because the user has already provided the exact amount and the target currency. My next step is to prepare the function call with the correct parameters. So, I'll structure the request with the amount as 500, from_currency as USD, and to_currency as EUR. \n\nThis will be sent in JSON format as specified, ensuring that the tool receives all the necessary information to perform the conversion accurately. Once the function is called, it should return the converted amount, which I can then present back to the user.\n&lt;/think&gt;&lt;tool_call&gt;\n{'name': 'convert_currency', 'arguments': {'amount': 500, 'from_currency': 'USD', 'to_currency': 'EUR'}}\n&lt;/tool_call&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;tool\n&lt;tool_response&gt;\n{'converted_amount': 420.5, 'from_currency': 'USD', 'to_currency': 'EUR'}\n&lt;/tool_response&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\nSure, 500 USD is approximately 420.50 Euros.&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;human\nThat's great! Now, can you convert 1000 Euros back to USD?&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\n&lt;think&gt;Okay, so I'm trying to figure out how to handle the next function call based on the conversation and the previous steps. Let me see...\n\nIn the conversation, the user first asked to convert 500 USD to Euros, and the model used the `convert_currency` function with the parameters amount=500, from_currency=USD, to_currency=EUR. Then, after getting the response, the model provided the converted amount. Now, the user is asking to convert 1000 Euros back to USD. \n\nSo, the user's request is clear: convert 1000 EUR to USD. Since the function `convert_currency` is designed to handle currency conversions, and it requires amount, from_currency, and to_currency as parameters, it makes sense to call this function again with the updated values. \n\nThe amount is 1000, from_currency should be EUR because the user is converting from Euros, and to_currency should be USD because they want the result in US Dollars. I don't need to make any assumptions here because all the required information is provided by the user. \n\nI'll structure the function call with these parameters. This should give the user the accurate converted amount from Euros to USD. Once the tool processes this, the model can present the result back to the user.\n&lt;/think&gt;&lt;tool_call&gt;\n{'name': 'convert_currency', 'arguments': {'amount': 1000, 'from_currency': 'EUR', 'to_currency': 'USD'}}\n&lt;/tool_call&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;tool\n&lt;tool_response&gt;\n{'converted_amount': 1190.0, 'from_currency': 'EUR', 'to_currency': 'USD'}\n&lt;/tool_response&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\n1000 Euros is approximately 1190.00 USD.&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;human\nThank you for your help!&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\nYou're welcome! If you have any other questions, feel free to ask.&lt;end_of_turn&gt;&lt;eos&gt;\n\n\n\n\n\nIn that case, we will take the start of one of the samples from the test set and hope that it will generate the expected output.\nSince we want to test the function-calling capacities of our newly fine-tuned model, the input will be a user message with the available tools.\n\n# this prompt is a sub-sample of one of the test set examples. In this example we start the generation after the model generation starts.\nprompt=\"\"\"&lt;bos&gt;&lt;start_of_turn&gt;human\nYou are a function calling AI model. You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:&lt;tools&gt; [{'type': 'function', 'function': {'name': 'convert_currency', 'description': 'Convert from one currency to another', 'parameters': {'type': 'object', 'properties': {'amount': {'type': 'number', 'description': 'The amount to convert'}, 'from_currency': {'type': 'string', 'description': 'The currency to convert from'}, 'to_currency': {'type': 'string', 'description': 'The currency to convert to'}}, 'required': ['amount', 'from_currency', 'to_currency']}}}, {'type': 'function', 'function': {'name': 'calculate_distance', 'description': 'Calculate the distance between two locations', 'parameters': {'type': 'object', 'properties': {'start_location': {'type': 'string', 'description': 'The starting location'}, 'end_location': {'type': 'string', 'description': 'The ending location'}}, 'required': ['start_location', 'end_location']}}}] &lt;/tools&gt;Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags as follows:\n&lt;tool_call&gt;\n{tool_call}\n&lt;/tool_call&gt;Also, before making a call to a function take the time to plan the function to take. Make that thinking process between &lt;think&gt;{your thoughts}&lt;/think&gt;\n\nHi, I need to convert 500 USD to Euros. Can you help me with that?&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\n&lt;think&gt;\"\"\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\ninputs = {k: v.to(\"cuda\") for k,v in inputs.items()}\noutputs = model.generate(**inputs,\n                         max_new_tokens=300,# Adapt as necessary\n                         do_sample=True,\n                         top_p=0.95,\n                         temperature=0.01,\n                         repetition_penalty=1.0,\n                         eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0]))\n\n&lt;bos&gt;&lt;start_of_turn&gt;human\nYou are a function calling AI model. You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:&lt;tools&gt; [{'type': 'function', 'function': {'name': 'convert_currency', 'description': 'Convert from one currency to another', 'parameters': {'type': 'object', 'properties': {'amount': {'type': 'number', 'description': 'The amount to convert'}, 'from_currency': {'type': 'string', 'description': 'The currency to convert from'}, 'to_currency': {'type': 'string', 'description': 'The currency to convert to'}}, 'required': ['amount', 'from_currency', 'to_currency']}}}, {'type': 'function', 'function': {'name': 'calculate_distance', 'description': 'Calculate the distance between two locations', 'parameters': {'type': 'object', 'properties': {'start_location': {'type': 'string', 'description': 'The starting location'}, 'end_location': {'type': 'string', 'description': 'The ending location'}}, 'required': ['start_location', 'end_location']}}}] &lt;/tools&gt;Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags as follows:\n&lt;tool_call&gt;\n{tool_call}\n&lt;/tool_call&gt;Also, before making a call to a function take the time to plan the function to take. Make that thinking process between &lt;think&gt;{your thoughts}&lt;/think&gt;\n\nHi, I need to convert 500 USD to Euros. Can you help me with that?&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\n&lt;think&gt;Okay, so the user is asking to convert 500 USD to Euros. I need to figure out how to respond using the available functions. Let me look at the tools provided. There's a function called convert_currency which does exactly that‚Äîit converts one currency to another. The parameters required are amount, from_currency, and to_currency. \n\nThe user provided the amount as 500, the source currency as USD, and the target currency as EUR. That fits perfectly with the function's parameters. I don't need to make any assumptions here because the user has given all the necessary details. \n\nSo, I should call the convert_currency function with these arguments. That should give the user the converted amount they need.\n&lt;/think&gt;&lt;tool_call&gt;\n{'name': 'convert_currency', 'arguments': {'amount': 500, 'from_currency': 'USD', 'to_currency': 'EUR'}}\n&lt;/tool_call&gt;&lt;end_of_turn&gt;&lt;eos&gt;",
    "crumbs": [
      "fc_agent NOTES",
      "FC_AGENT üëÄ"
    ]
  },
  {
    "objectID": "index.html#dependencies",
    "href": "index.html#dependencies",
    "title": "FC_AGENT üëÄ",
    "section": "",
    "text": "! pip install -q -U bitsandbytes   # for quantization\n! pip install -q -U peft           # for LoRA adapters\n! pip install -q -U trl            # for loading the model\n! pip install -q -U tensorboardX   # for loading and using the fine-tuning dataset\n! pip install -q wandb             # for the trainer class\n\n\nfrom enum import Enum\nfrom functools import partial\nimport pandas as pd\nimport torch\nimport json\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\nfrom datasets import load_dataset\nfrom trl import SFTConfig, SFTTrainer\nfrom peft import LoraConfig, TaskType\n\nseed = 42\nset_seed(seed)\n\nimport os\n\n# Put your HF Token here\nos.environ['HF_TOKEN']=\"hf_xxxxxxx\" # the token should have write access",
    "crumbs": [
      "fc_agent NOTES",
      "FC_AGENT üëÄ"
    ]
  },
  {
    "objectID": "index.html#preprocessing-of-data",
    "href": "index.html#preprocessing-of-data",
    "title": "FC_AGENT üëÄ",
    "section": "",
    "text": "We need to format the inputs into what we want the model to learn.\nDataset for this fine tuning contains: ‚ÄúNousResearch/hermes-function-calling-v1‚Äù (A popular dataset for function calling) and some of the dataset new thinking step computer from deepseek-ai/DeepSeek-R1-Distill-Qwen-32B.\nDefault chat_template of gemma-2-2B does not contain tool calls Conversation format needs to be changed to include tool calls.\n‚ÄúNousResearch/hermes-function-calling-v1‚Äù does not include a ‚Äúthinking‚Äù step. In Function-Calling, such a step is optional, but recent work‚Äîlike the deepseek model or the paper ‚ÄúTest-Time Compute‚Äù ‚Äî suggests that giving an LLM time to ‚Äúthink‚Äù before it answers (or in this case, before taking an action) can significantly improve model performance.\n\nmodel_name = \"google/gemma-2-2b-it\"\ndataset_name = \"Jofthomas/hermes-function-calling-thinking-V1\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ntokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '&lt;start_of_turn&gt;' + message['role'] + '\\n' + message['content'] | trim + '&lt;end_of_turn&gt;&lt;eos&gt;\\n' }}{% endfor %}{% if add_generation_prompt %}{{'&lt;start_of_turn&gt;model\\n'}}{% endif %}\"\n\n\ndef preprocess(sample):\n      messages = sample[\"messages\"]\n      first_message = messages[0]\n\n      # Instead of adding a system message, we merge the content into the first user message\n      if first_message[\"role\"] == \"system\":\n          system_message_content = first_message[\"content\"]\n          # Merge system content with the first user message\n          messages[1][\"content\"] = system_message_content + \"Also, before making a call to a function take the time to plan the function to take. Make that thinking process between &lt;think&gt;{your thoughts}&lt;/think&gt;\\n\\n\" + messages[1][\"content\"]\n          # Remove the system message from the conversation\n          messages.pop(0)\n\n      return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n\n\n\ndataset = load_dataset(dataset_name)\ndataset = dataset.rename_column(\"conversations\", \"messages\")\n\nExample of the conversation format:\n&lt;bos&gt;&lt;start_of_turn&gt;human\nYou are a function calling AI model. You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:&lt;tools&gt; [{'type': 'function', 'function': {'name': 'get_news_headlines', 'description': 'Get the latest news headlines', 'parameters': {'type': 'object', 'properties': {'country': {'type': 'string', 'description': 'The country for which headlines are needed'}}, 'required': ['country']}}}, {'type': 'function', 'function': {'name': 'search_recipes', 'description': 'Search for recipes based on ingredients', 'parameters': {'type': 'object', 'properties': {'ingredients': {'type': 'array', 'items': {'type': 'string'}, 'description': 'The list of ingredients'}}, 'required': ['ingredients']}}}] &lt;/tools&gt;Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags as follows:\n&lt;tool_call&gt;\n{tool_call}\n&lt;/tool_call&gt;Also, before making a call to a function take the time to plan the function to take. Make that thinking process between &lt;think&gt;{your thoughts}&lt;/think&gt;\n\nCan you get me the latest news headlines for the United States?&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\n&lt;think&gt;Okay, so the user asked for the latest news headlines in the United States. I need to figure out how to respond appropriately using the tools available.\n\nLooking at the tools provided, there's a function called get_news_headlines. That seems perfect for this situation. It requires a country code, which the user specified as the United States.\n\nSince the user is asking specifically about the US, I'll use the country parameter as 'United States' in the function call. This should fetch the relevant news for them.\n\nI should make sure to format the response correctly as a JSON object with the function name and arguments.\n&lt;/think&gt;&lt;tool_call&gt;\n{'name': 'get_news_headlines', 'arguments': {'country': 'United States'}}\n&lt;/tool_call&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;tool\n&lt;tool_response&gt;\n{'headlines': ['US President signs new bill into law', 'Major earthquake hits California', 'NASA announces new Mars mission', 'Tech giant releases new smartphone', 'US economy shows signs of recovery']}\n&lt;/tool_response&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\nHere are the latest news headlines for the United States:\n1. US President signs new bill into law\n2. Major earthquake hits California\n3. NASA announces new Mars mission\n4. Tech giant releases new smartphone\n5. US economy shows signs of recovery&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;human\nThat's interesting. Can you get me the news for France now?&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\n&lt;think&gt;Okay, the user initially asked for the latest US news, so I used get_news_headlines with 'United States'. Now the user is asking for France, so I'll call the same function with 'France' as the country. That makes sense because the function is designed for any country.\n&lt;/think&gt;&lt;tool_call&gt;\n{'name': 'get_news_headlines', 'arguments': {'country': 'France'}}\n&lt;/tool_call&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;tool\n&lt;tool_response&gt;\n{'headlines': ['French President announces new environmental policy', 'Paris Fashion Week highlights', 'France wins World Cup qualifier', 'New culinary trend sweeps across France', 'French tech startup raises millions in funding']}\n&lt;/tool_response&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\nHere are the latest news headlines for France:\n1. French President announces new environmental policy\n2. Paris Fashion Week highlights\n3. France wins World Cup qualifier\n4. New culinary trend sweeps across France\n5. French tech startup raises millions in funding&lt;end_of_turn&gt;&lt;eos&gt;\nTokenizer splits text into sub-words by default. This is not what we want for our new special tokens!\nWhile we segmented our example using &lt;think&gt;, &lt;tool_call&gt;, and &lt;tool_response&gt;, the tokenizer does not yet treat them as whole tokens‚Äîit still tries to break them down into smaller pieces. To ensure the model correctly interprets our new format, we must add these tokens to our tokenizer.\nAdditionally, since we changed the chat_template in our preprocess function to format conversations as messages within a prompt, we also need to modify the chat_template in the tokenizer to reflect these changes.\n\ndataset = dataset.map(preprocess, remove_columns=\"messages\")\ndataset = dataset[\"train\"].train_test_split(0.1)\nprint(dataset)\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 3213\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 357\n    })\n})",
    "crumbs": [
      "fc_agent NOTES",
      "FC_AGENT üëÄ"
    ]
  },
  {
    "objectID": "index.html#inputs",
    "href": "index.html#inputs",
    "title": "FC_AGENT üëÄ",
    "section": "",
    "text": "Let‚Äôs manually look at what an input looks like !\nIn this example we have :\n\nA User message containing the necessary information with the list of available tools inbetween &lt;tools&gt;&lt;/tools&gt; then the user query, here: \"Can you get me the latest news headlines for the United States?\"\nAn Assistant message here called ‚Äúmodel‚Äù to fit the criterias from gemma models containing two new phases, a ‚Äúthinking‚Äù phase contained in &lt;think&gt;&lt;/think&gt; and an ‚ÄúAct‚Äù phase contained in &lt;tool_call&gt;&lt;/&lt;tool_call&gt;.\nIf the model contains a &lt;tools_call&gt;, we will append the result of this action in a new ‚ÄúTool‚Äù message containing a &lt;tool_response&gt;&lt;/tool_response&gt; with the answer from the tool.\n\n\n# Let's look at how we formatted the dataset\nprint(dataset[\"train\"][8][\"text\"])\n\n&lt;bos&gt;&lt;start_of_turn&gt;human\nYou are a function calling AI model. You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:&lt;tools&gt; [{'type': 'function', 'function': {'name': 'get_news_headlines', 'description': 'Get the latest news headlines', 'parameters': {'type': 'object', 'properties': {'country': {'type': 'string', 'description': 'The country for which headlines are needed'}}, 'required': ['country']}}}, {'type': 'function', 'function': {'name': 'search_recipes', 'description': 'Search for recipes based on ingredients', 'parameters': {'type': 'object', 'properties': {'ingredients': {'type': 'array', 'items': {'type': 'string'}, 'description': 'The list of ingredients'}}, 'required': ['ingredients']}}}] &lt;/tools&gt;Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags as follows:\n&lt;tool_call&gt;\n{tool_call}\n&lt;/tool_call&gt;Also, before making a call to a function take the time to plan the function to take. Make that thinking process between &lt;think&gt;{your thoughts}&lt;/think&gt;\n\nCan you get me the latest news headlines for the United States?&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\n&lt;think&gt;Okay, so the user asked for the latest news headlines in the United States. I need to figure out how to respond appropriately using the tools available.\n\nLooking at the tools provided, there's a function called get_news_headlines. That seems perfect for this situation. It requires a country code, which the user specified as the United States.\n\nSince the user is asking specifically about the US, I'll use the country parameter as 'United States' in the function call. This should fetch the relevant news for them.\n\nI should make sure to format the response correctly as a JSON object with the function name and arguments.\n&lt;/think&gt;&lt;tool_call&gt;\n{'name': 'get_news_headlines', 'arguments': {'country': 'United States'}}\n&lt;/tool_call&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;tool\n&lt;tool_response&gt;\n{'headlines': ['US President signs new bill into law', 'Major earthquake hits California', 'NASA announces new Mars mission', 'Tech giant releases new smartphone', 'US economy shows signs of recovery']}\n&lt;/tool_response&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\nHere are the latest news headlines for the United States:\n1. US President signs new bill into law\n2. Major earthquake hits California\n3. NASA announces new Mars mission\n4. Tech giant releases new smartphone\n5. US economy shows signs of recovery&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;human\nThat's interesting. Can you get me the news for France now?&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\n&lt;think&gt;Okay, the user initially asked for the latest US news, so I used get_news_headlines with 'United States'. Now the user is asking for France, so I'll call the same function with 'France' as the country. That makes sense because the function is designed for any country.\n&lt;/think&gt;&lt;tool_call&gt;\n{'name': 'get_news_headlines', 'arguments': {'country': 'France'}}\n&lt;/tool_call&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;tool\n&lt;tool_response&gt;\n{'headlines': ['French President announces new environmental policy', 'Paris Fashion Week highlights', 'France wins World Cup qualifier', 'New culinary trend sweeps across France', 'French tech startup raises millions in funding']}\n&lt;/tool_response&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\nHere are the latest news headlines for France:\n1. French President announces new environmental policy\n2. Paris Fashion Week highlights\n3. France wins World Cup qualifier\n4. New culinary trend sweeps across France\n5. French tech startup raises millions in funding&lt;end_of_turn&gt;&lt;eos&gt;\n\n\n\n\n# Sanity check\nprint(tokenizer.pad_token)\nprint(tokenizer.eos_token)\n\n&lt;pad&gt;\n&lt;eos&gt;",
    "crumbs": [
      "fc_agent NOTES",
      "FC_AGENT üëÄ"
    ]
  },
  {
    "objectID": "index.html#modify-the-tokenizer",
    "href": "index.html#modify-the-tokenizer",
    "title": "FC_AGENT üëÄ",
    "section": "",
    "text": "Tokenizer splits text into sub-words by default. This is not what we want for our new special tokens!\nWhile we segmented our example using &lt;think&gt;, &lt;tool_call&gt;, and &lt;tool_response&gt;, the tokenizer does not yet treat them as whole tokens‚Äîit still tries to break them down into smaller pieces. To ensure the model correctly interprets our new format, we must add these tokens to our tokenizer.\nAdditionally, since we changed the chat_template in our preprocess function to format conversations as messages within a prompt, we also need to modify the chat_template in the tokenizer to reflect these changes.\n\nclass ChatmlSpecialTokens(str, Enum):\n    tools = \"&lt;tools&gt;\"\n    eotools = \"&lt;/tools&gt;\"\n    think = \"&lt;think&gt;\"\n    eothink = \"&lt;/think&gt;\"\n    tool_call=\"&lt;tool_call&gt;\"\n    eotool_call=\"&lt;/tool_call&gt;\"\n    tool_response=\"&lt;tool_reponse&gt;\"\n    eotool_response=\"&lt;/tool_reponse&gt;\"\n    pad_token = \"&lt;pad&gt;\"\n    eos_token = \"&lt;eos&gt;\"\n    @classmethod\n    def list(cls):\n        return [c.value for c in cls]\n\ntokenizer = AutoTokenizer.from_pretrained(\n        model_name,\n        pad_token=ChatmlSpecialTokens.pad_token.value,\n        additional_special_tokens=ChatmlSpecialTokens.list()\n    )\ntokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '&lt;start_of_turn&gt;' + message['role'] + '\\n' + message['content'] | trim + '&lt;end_of_turn&gt;&lt;eos&gt;\\n' }}{% endfor %}{% if add_generation_prompt %}{{'&lt;start_of_turn&gt;model\\n'}}{% endif %}\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                             attn_implementation='eager',\n                                             device_map=\"auto\")\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(torch.bfloat16)\n\n\n\n\nThe new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n\n\nGemma2ForCausalLM(\n  (model): Gemma2Model(\n    (embed_tokens): Embedding(256008, 2304, padding_idx=0)\n    (layers): ModuleList(\n      (0-25): 26 x Gemma2DecoderLayer(\n        (self_attn): Gemma2Attention(\n          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n          (rotary_emb): Gemma2RotaryEmbedding()\n        )\n        (mlp): Gemma2MLP(\n          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n          (act_fn): PytorchGELUTanh()\n        )\n        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n      )\n    )\n    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n  )\n  (lm_head): Linear(in_features=2304, out_features=256008, bias=False)\n)",
    "crumbs": [
      "fc_agent NOTES",
      "FC_AGENT üëÄ"
    ]
  },
  {
    "objectID": "index.html#lora",
    "href": "index.html#lora",
    "title": "FC_AGENT üëÄ",
    "section": "",
    "text": "This is we are going to define the parameter of our adapter. Those a the most important parameters in LoRA as they define the size and importance of the adapters we are training.\n\nfrom peft import LoraConfig\n\n# TODO: Configure LoRA parameters\n# r: rank dimension for LoRA update matrices (smaller = more compression)\nrank_dimension = 16\n# lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)\nlora_alpha = 64\n# lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)\nlora_dropout = 0.05\n\npeft_config = LoraConfig(r=rank_dimension,\n                         lora_alpha=lora_alpha,\n                         lora_dropout=lora_dropout,\n                         target_modules=[\"gate_proj\",\"q_proj\",\"lm_head\",\"o_proj\",\"k_proj\",\"embed_tokens\",\"down_proj\",\"up_proj\",\"v_proj\"], # wich layer in the transformers do we target ?\n                         task_type=TaskType.CAUSAL_LM)",
    "crumbs": [
      "fc_agent NOTES",
      "FC_AGENT üëÄ"
    ]
  },
  {
    "objectID": "index.html#trainer-and-the-fine-tuning-hyperparameters",
    "href": "index.html#trainer-and-the-fine-tuning-hyperparameters",
    "title": "FC_AGENT üëÄ",
    "section": "",
    "text": "Trainer, the class that we use to fine-tune our model and the hyperparameters.\n\nusername=\"Jofthomas\"# REPLCAE with your Hugging Face username\noutput_dir = \"gemma-2-2B-it-thinking-function_calling-V0\" # The directory where the trained model checkpoints, logs, and other artifacts will be saved. It will also be the default name of the model when pushed to the hub if not redefined later.\nper_device_train_batch_size = 1\nper_device_eval_batch_size = 1\ngradient_accumulation_steps = 4\nlogging_steps = 5\nlearning_rate = 1e-4 # The initial learning rate for the optimizer.\n\nmax_grad_norm = 1.0\nnum_train_epochs=1\nwarmup_ratio = 0.1\nlr_scheduler_type = \"cosine\"\nmax_seq_length = 1500\n\ntraining_arguments = SFTConfig(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    per_device_eval_batch_size=per_device_eval_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    save_strategy=\"no\",\n    eval_strategy=\"epoch\",\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    max_grad_norm=max_grad_norm,\n    weight_decay=0.1,\n    warmup_ratio=warmup_ratio,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\",\n    bf16=True,\n    hub_private_repo=False,\n    push_to_hub=False,\n    num_train_epochs=num_train_epochs,\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    packing=True,\n    max_seq_length=max_seq_length,\n)\n\nAs Trainer, we use the SFTTrainer which is a Supervised Fine-Tuning Trainer.\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_arguments,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    processing_class=tokenizer,\n    peft_config=peft_config,\n)\n\n/home/user/miniconda/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:543: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we launch the training üî•. Perfect time for you to pause and grab a coffee ‚òï.\n\ntrainer.train()\ntrainer.save_model()\n\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n\n\n\n    \n      \n      \n      [12/12 00:24, Epoch 1/1]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\n\n\n\n\n1\n1.236800\n1.240833\n\n\n\n\n\n\n/home/user/miniconda/lib/python3.9/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")",
    "crumbs": [
      "fc_agent NOTES",
      "FC_AGENT üëÄ"
    ]
  },
  {
    "objectID": "index.html#test",
    "href": "index.html#test",
    "title": "FC_AGENT üëÄ",
    "section": "",
    "text": "To so:\n\nLoad the adapter from the hub !\nLoad the base model : ‚Äúgoogle/gemma-2-2b-it‚Äù from the hub\nResize the model to with the new tokens we introduced !\n\n\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom datasets import load_dataset\nimport torch\n\nbnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n            bnb_4bit_use_double_quant=True,\n        )\n\npeft_model_id = f\"{username}/{output_dir}\" # replace with your newly trained adapter\ndevice = \"auto\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n                                             device_map=\"auto\",\n                                             )\ntokenizer = AutoTokenizer.from_pretrained(peft_model_id)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel = PeftModel.from_pretrained(model, peft_model_id)\nmodel.to(torch.bfloat16)\nmodel.eval()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Gemma2ForCausalLM(\n      (model): Gemma2Model(\n        (embed_tokens): lora.Embedding(\n          (base_layer): Embedding(256006, 2304, padding_idx=0)\n          (lora_dropout): ModuleDict(\n            (default): Dropout(p=0.05, inplace=False)\n          )\n          (lora_A): ModuleDict()\n          (lora_B): ModuleDict()\n          (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 16x256006 (cuda:0)])\n          (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 2304x16 (cuda:0)])\n          (lora_magnitude_vector): ModuleDict()\n        )\n        (layers): ModuleList(\n          (0-25): 26 x Gemma2DecoderLayer(\n            (self_attn): Gemma2Attention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=2304, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear(\n                (base_layer): Linear(in_features=2304, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=2304, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=2304, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2304, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): Gemma2RotaryEmbedding()\n            )\n            (mlp): Gemma2MLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=2304, out_features=9216, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=9216, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=2304, out_features=9216, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=9216, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=9216, out_features=2304, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=9216, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2304, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): PytorchGELUTanh()\n            )\n            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n          )\n        )\n        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n      )\n      (lm_head): lora.Linear(\n        (base_layer): Linear(in_features=2304, out_features=256006, bias=False)\n        (lora_dropout): ModuleDict(\n          (default): Dropout(p=0.05, inplace=False)\n        )\n        (lora_A): ModuleDict(\n          (default): Linear(in_features=2304, out_features=16, bias=False)\n        )\n        (lora_B): ModuleDict(\n          (default): Linear(in_features=16, out_features=256006, bias=False)\n        )\n        (lora_embedding_A): ParameterDict()\n        (lora_embedding_B): ParameterDict()\n        (lora_magnitude_vector): ModuleDict()\n      )\n    )\n  )\n)\n\n\n\nprint(dataset[\"test\"][8][\"text\"])\n\n&lt;bos&gt;&lt;start_of_turn&gt;human\nYou are a function calling AI model. You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:&lt;tools&gt; [{'type': 'function', 'function': {'name': 'convert_currency', 'description': 'Convert from one currency to another', 'parameters': {'type': 'object', 'properties': {'amount': {'type': 'number', 'description': 'The amount to convert'}, 'from_currency': {'type': 'string', 'description': 'The currency to convert from'}, 'to_currency': {'type': 'string', 'description': 'The currency to convert to'}}, 'required': ['amount', 'from_currency', 'to_currency']}}}, {'type': 'function', 'function': {'name': 'calculate_distance', 'description': 'Calculate the distance between two locations', 'parameters': {'type': 'object', 'properties': {'start_location': {'type': 'string', 'description': 'The starting location'}, 'end_location': {'type': 'string', 'description': 'The ending location'}}, 'required': ['start_location', 'end_location']}}}] &lt;/tools&gt;Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags as follows:\n&lt;tool_call&gt;\n{tool_call}\n&lt;/tool_call&gt;Also, before making a call to a function take the time to plan the function to take. Make that thinking process between &lt;think&gt;{your thoughts}&lt;/think&gt;\n\nHi, I need to convert 500 USD to Euros. Can you help me with that?&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\n&lt;think&gt;Alright, so I'm looking at this scenario where the user needs help converting 500 USD to Euros. Let's break it down step by step to understand how to approach this.\n\nFirst, the user has asked for conversion from USD to EUR. I know there's a specific tool available for currency conversion called `convert_currency`. This tool requires three parameters: amount, from_currency, and to_currency. In the user's message, they've provided all necessary details‚Äî500 USD and the desire to convert to Euros. \n\nI don't need to make any assumptions here because the user has already provided the exact amount and the target currency. My next step is to prepare the function call with the correct parameters. So, I'll structure the request with the amount as 500, from_currency as USD, and to_currency as EUR. \n\nThis will be sent in JSON format as specified, ensuring that the tool receives all the necessary information to perform the conversion accurately. Once the function is called, it should return the converted amount, which I can then present back to the user.\n&lt;/think&gt;&lt;tool_call&gt;\n{'name': 'convert_currency', 'arguments': {'amount': 500, 'from_currency': 'USD', 'to_currency': 'EUR'}}\n&lt;/tool_call&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;tool\n&lt;tool_response&gt;\n{'converted_amount': 420.5, 'from_currency': 'USD', 'to_currency': 'EUR'}\n&lt;/tool_response&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\nSure, 500 USD is approximately 420.50 Euros.&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;human\nThat's great! Now, can you convert 1000 Euros back to USD?&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\n&lt;think&gt;Okay, so I'm trying to figure out how to handle the next function call based on the conversation and the previous steps. Let me see...\n\nIn the conversation, the user first asked to convert 500 USD to Euros, and the model used the `convert_currency` function with the parameters amount=500, from_currency=USD, to_currency=EUR. Then, after getting the response, the model provided the converted amount. Now, the user is asking to convert 1000 Euros back to USD. \n\nSo, the user's request is clear: convert 1000 EUR to USD. Since the function `convert_currency` is designed to handle currency conversions, and it requires amount, from_currency, and to_currency as parameters, it makes sense to call this function again with the updated values. \n\nThe amount is 1000, from_currency should be EUR because the user is converting from Euros, and to_currency should be USD because they want the result in US Dollars. I don't need to make any assumptions here because all the required information is provided by the user. \n\nI'll structure the function call with these parameters. This should give the user the accurate converted amount from Euros to USD. Once the tool processes this, the model can present the result back to the user.\n&lt;/think&gt;&lt;tool_call&gt;\n{'name': 'convert_currency', 'arguments': {'amount': 1000, 'from_currency': 'EUR', 'to_currency': 'USD'}}\n&lt;/tool_call&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;tool\n&lt;tool_response&gt;\n{'converted_amount': 1190.0, 'from_currency': 'EUR', 'to_currency': 'USD'}\n&lt;/tool_response&gt;&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\n1000 Euros is approximately 1190.00 USD.&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;human\nThank you for your help!&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\nYou're welcome! If you have any other questions, feel free to ask.&lt;end_of_turn&gt;&lt;eos&gt;\n\n\n\n\n\nIn that case, we will take the start of one of the samples from the test set and hope that it will generate the expected output.\nSince we want to test the function-calling capacities of our newly fine-tuned model, the input will be a user message with the available tools.\n\n# this prompt is a sub-sample of one of the test set examples. In this example we start the generation after the model generation starts.\nprompt=\"\"\"&lt;bos&gt;&lt;start_of_turn&gt;human\nYou are a function calling AI model. You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:&lt;tools&gt; [{'type': 'function', 'function': {'name': 'convert_currency', 'description': 'Convert from one currency to another', 'parameters': {'type': 'object', 'properties': {'amount': {'type': 'number', 'description': 'The amount to convert'}, 'from_currency': {'type': 'string', 'description': 'The currency to convert from'}, 'to_currency': {'type': 'string', 'description': 'The currency to convert to'}}, 'required': ['amount', 'from_currency', 'to_currency']}}}, {'type': 'function', 'function': {'name': 'calculate_distance', 'description': 'Calculate the distance between two locations', 'parameters': {'type': 'object', 'properties': {'start_location': {'type': 'string', 'description': 'The starting location'}, 'end_location': {'type': 'string', 'description': 'The ending location'}}, 'required': ['start_location', 'end_location']}}}] &lt;/tools&gt;Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags as follows:\n&lt;tool_call&gt;\n{tool_call}\n&lt;/tool_call&gt;Also, before making a call to a function take the time to plan the function to take. Make that thinking process between &lt;think&gt;{your thoughts}&lt;/think&gt;\n\nHi, I need to convert 500 USD to Euros. Can you help me with that?&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\n&lt;think&gt;\"\"\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\ninputs = {k: v.to(\"cuda\") for k,v in inputs.items()}\noutputs = model.generate(**inputs,\n                         max_new_tokens=300,# Adapt as necessary\n                         do_sample=True,\n                         top_p=0.95,\n                         temperature=0.01,\n                         repetition_penalty=1.0,\n                         eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0]))\n\n&lt;bos&gt;&lt;start_of_turn&gt;human\nYou are a function calling AI model. You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:&lt;tools&gt; [{'type': 'function', 'function': {'name': 'convert_currency', 'description': 'Convert from one currency to another', 'parameters': {'type': 'object', 'properties': {'amount': {'type': 'number', 'description': 'The amount to convert'}, 'from_currency': {'type': 'string', 'description': 'The currency to convert from'}, 'to_currency': {'type': 'string', 'description': 'The currency to convert to'}}, 'required': ['amount', 'from_currency', 'to_currency']}}}, {'type': 'function', 'function': {'name': 'calculate_distance', 'description': 'Calculate the distance between two locations', 'parameters': {'type': 'object', 'properties': {'start_location': {'type': 'string', 'description': 'The starting location'}, 'end_location': {'type': 'string', 'description': 'The ending location'}}, 'required': ['start_location', 'end_location']}}}] &lt;/tools&gt;Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags as follows:\n&lt;tool_call&gt;\n{tool_call}\n&lt;/tool_call&gt;Also, before making a call to a function take the time to plan the function to take. Make that thinking process between &lt;think&gt;{your thoughts}&lt;/think&gt;\n\nHi, I need to convert 500 USD to Euros. Can you help me with that?&lt;end_of_turn&gt;&lt;eos&gt;\n&lt;start_of_turn&gt;model\n&lt;think&gt;Okay, so the user is asking to convert 500 USD to Euros. I need to figure out how to respond using the available functions. Let me look at the tools provided. There's a function called convert_currency which does exactly that‚Äîit converts one currency to another. The parameters required are amount, from_currency, and to_currency. \n\nThe user provided the amount as 500, the source currency as USD, and the target currency as EUR. That fits perfectly with the function's parameters. I don't need to make any assumptions here because the user has given all the necessary details. \n\nSo, I should call the convert_currency function with these arguments. That should give the user the converted amount they need.\n&lt;/think&gt;&lt;tool_call&gt;\n{'name': 'convert_currency', 'arguments': {'amount': 500, 'from_currency': 'USD', 'to_currency': 'EUR'}}\n&lt;/tool_call&gt;&lt;end_of_turn&gt;&lt;eos&gt;",
    "crumbs": [
      "fc_agent NOTES",
      "FC_AGENT üëÄ"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Fevzi KILAS",
    "crumbs": [
      "fc_agent NOTES",
      "About"
    ]
  }
]